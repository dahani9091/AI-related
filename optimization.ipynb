{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm,inv\n",
    "from scipy.misc import derivative\n",
    "from numdifftools import Gradient,Hessian\n",
    "import numdifftools as nd\n",
    "from scipy.optimize import linprog,minimize_scalar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Linear search :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bissection(f,a,b,tol):\n",
    "    a,b = min(a,b), max(a,b)\n",
    "    try :\n",
    "        while (abs(b-a)>tol):\n",
    "            c = (a + b) / 2\n",
    "            if (derivative(f,c) == 0 and derivative(f,c, n=2) > 0):\n",
    "                return c\n",
    "            elif (derivative(f, c) <= 0):\n",
    "                a = c\n",
    "            else:\n",
    "                b = c\n",
    "        return [a,b]\n",
    "    except:\n",
    "        print(\"Erreur : Verifier les bornes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fausse_position(f,x0,x1,tol):\n",
    "    if(x0 == x1):\n",
    "        print(\"Erreur : x0 = x1 ! ce n'est pas une intervalle. \")\n",
    "    x_bef = x0\n",
    "    x_prev = x1\n",
    "    while ((abs(x_prev-x_bef)>=tol) or (abs(derivative(f,x_prev))>= tol)):\n",
    "        d = - derivative(f,x_prev) * ((x_bef - x_prev)/(derivative(f,x_bef)-derivative(f,x_prev)))\n",
    "        x_bef = x_prev\n",
    "        x_prev = x_bef + d\n",
    "    return x_prev    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def raphsonNewton ( fct , x0 , tol ):\n",
    "    x_k = x0\n",
    "    if (derivative(fct , x_k,n = 2) != 0):\n",
    "            d = -(derivative(fct , x_k,n = 1) / derivative(fct , x_k,n = 2))\n",
    "            x_kk = x_k\n",
    "            x_k = x_kk + d\n",
    "    else :\n",
    "            print(\"Dérivée seconde de f est nulle!! Newton est inutile dans ce cas\")\n",
    "            return x_k\n",
    "    while (abs(x_kk - x_k)> tol or abs(derivative(fct , x_k,n = 1)) > tol ):\n",
    "        if (derivative(fct , x_k,n = 2) != 0):\n",
    "            d = -(derivative(fct , x_k,n = 1) / derivative(fct , x_k,n = 2))\n",
    "            x_kk = x_k\n",
    "            x_k = x_kk + d\n",
    "        else :\n",
    "            print(\"Dérivée seconde de f est nulle!! Newton est inutile dans ce cas\")\n",
    "            return x_k\n",
    "    return x_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.18350341368301804"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phi = lambda x: x**3 + 3*x**2 -1\n",
    "bissection(phi,1,-1,0.001)\n",
    "raphsonNewton(phi,0.5,0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.00015345018797644286"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fuc(x):\n",
    "    return x**4 + 2*(x**2) +1\n",
    "fausse_position(fuc,0.4,-0.2,0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pas approche \n",
    "def armijo(phi,eps = 0.5,alpha0 = 1):\n",
    "    alpha = alpha0\n",
    "    while (phi(alpha) <= phi(0) + eps* alpha * derivative(phi, 0)):\n",
    "        alpha = 2 * alpha\n",
    "    while (phi(alpha) > phi(0) + eps *alpha* derivative(phi, 0)):\n",
    "        alpha = alpha / 2\n",
    "    return alpha\n",
    "def goldstein(phi,eps = 0.1,a0 = 0,alpha0 = 1 , t = 2):\n",
    "    alpha = alpha0\n",
    "    a = a0\n",
    "    b_inf = 1000\n",
    "    b = b_inf\n",
    "    while (True):\n",
    "        if(phi(alpha) <= (phi(0) + eps *alpha*derivative(phi,0))):\n",
    "            if(phi(alpha) >= (phi(0)+(1-eps)*alpha)*derivative(phi,0)):\n",
    "                return alpha\n",
    "            else :\n",
    "                a = alpha\n",
    "            if b < b_inf :\n",
    "                print(\"hi\")\n",
    "                alpha = (a+b)/2\n",
    "            else  : #b >= b_inf:\n",
    "                alpha = t*alpha\n",
    "        else :\n",
    "            b = alpha\n",
    "            alpha = (a+b)/2\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.450580596923828e-09 7.450580596923828e-09\n"
     ]
    }
   ],
   "source": [
    "print(armijo(fuc),goldstein(fuc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Constrained optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 - Gradient descent Algorithm :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(f,x0,tol):\n",
    "    x_prev = x0\n",
    "    x_next = x0\n",
    "    while(True):\n",
    "        phi = lambda a : f(x_prev-a*Gradient(f)(x_prev))\n",
    "        alpha = armijo(phi)\n",
    "        x_next = x_prev - alpha * Gradient(f)(x_prev)\n",
    "        if(norm(Gradient(f)(x_next))>tol or norm(x_next-x_prev)>tol ):\n",
    "            break\n",
    "    return x_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.88498131e-15 2.22044605e-16]\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    return 0.5*(x[0]**2 +x[1]**2)\n",
    "x0 = np.array([2,1],dtype = float)\n",
    "print(gradient_descent(f,x0,0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 - Newton Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_pos_def(H):\n",
    "    return np.all(np.linalg.eigvals(H) > 0)\n",
    "\n",
    "def newton(f, x0, tol):\n",
    "    x = x0\n",
    "    d = np.dot(inv(Hessian(f)(x)), Gradient(f)(x))\n",
    "    while norm(d) > tol:\n",
    "        phi = lambda alpha: f(x - alpha * d)\n",
    "        alpha = max(0, raphsonNewton(phi, 1, tol))\n",
    "        x = x - alpha * d\n",
    "        print(x)\n",
    "        d = np.dot(inv(Hessian(f)(x)), Gradient(f)(x))\n",
    "    return x\n",
    "def Newton_alg(f , x0 , tol):\n",
    "    x = x0\n",
    "    eps = 0.001\n",
    "    d = np.dot(inv(Hessian(f)(x)) , Gradient(f)(x))\n",
    "    while (norm(d)>tol):\n",
    "        phi = lambda alpha : f(x - Gradient(f)(x) * alpha)\n",
    "        alpha = max(0,armijo(phi))\n",
    "        x = x - alpha * d\n",
    "        if (is_pos_def(Hessian(f)(x))):\n",
    "            d =  inv(Hessian(f)(x)) @ Gradient(f)(x)\n",
    "        else: \n",
    "            d =  inv(eps * np.eye(len(x0))+Hessian(f)(x)) @ Gradient(f)(x)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7.54951657e-15 1.99840144e-15]\n"
     ]
    }
   ],
   "source": [
    "x0 = np.array([2,1])\n",
    "print(Newton_alg(f , x0 , 0.01))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Unconstrained optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 - Feasible Directions algorithm (directions réalisables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feasible_directions(f,A,b,x0):\n",
    "    x = x0\n",
    "    while(True):\n",
    "        # calculate the direction #\n",
    "        Ik = []\n",
    "        for i in range(0,len(A)):\n",
    "            if abs(np.vdot(A[i,:],x) - b[i] )< 0.0001 :\n",
    "                Ik.append(i)\n",
    "        #print(\"Ik :\",Ik)\n",
    "        c = Gradient(f)(x)\n",
    "        if len(Ik) == 0 :\n",
    "            d = linprog(c,bounds=(-1,1)).x\n",
    "        else :\n",
    "            A_prog = A[Ik,:]\n",
    "            b_prog = np.zeros(len(Ik))\n",
    "            d = linprog(c,A_ub = A_prog,b_ub = b_prog,bounds=(-1,1)).x\n",
    "        #print(\"direction \",d)\n",
    "        # calculate the learning rate  alpha_bar - alpha #\n",
    "        alphas_set = []\n",
    "        for i in range(0,len(A)):\n",
    "            if (np.vdot(A[i,:],d)>0) and (i not in Ik) :\n",
    "                alph = (b[i]-np.vdot(A[i,:],x)) / np.vdot(A[i,:],d)\n",
    "                alphas_set.append(alph)\n",
    "        alpha_bar = min(alphas_set)\n",
    "        phi = lambda alpha : f(x + alpha * d)\n",
    "        opt_alpha = minimize_scalar(phi, bounds=(0, alpha_bar), method='bounded').x\n",
    "        #print(\"optimum alpha: \",opt_alpha)\n",
    "        # increment x\n",
    "        x = x + opt_alpha * d\n",
    "        #print(\"x :\", x)\n",
    "        if np.vdot(Gradient(f)(x),d)>0 :\n",
    "            break\n",
    "    return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "direction  [ 1. -0.]\n",
      "1\n",
      "direction  [ 1. -0.]\n",
      "[3.49719188e-06 2.00000596e+00]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([\n",
    "    [-1, 1],\n",
    "    [ 1, 1],\n",
    "    [ 0,-1]\n",
    "],dtype = float)\n",
    "\n",
    "b = np.array([7,5,-2])\n",
    "x0 = np.array([-2,3])\n",
    "x = feasible_directions(f,A,b,x0)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
